{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.MORO OPTIMIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Generate random policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 0.000001) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1000) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = functools.partial(robustness, SMALLER, 150000000)#THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem_formulation import get_model_for_problem_formulation\n",
    "\n",
    "model = get_model_for_problem_formulation(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] pool started with 8 workers\n",
      "[MainProcess/INFO] performing 200 scenarios * 4 policies * 1 model(s) = 800 experiments\n",
      " 45%|██████████████████                      | 360/800 [01:56<01:31,  4.83it/s]"
     ]
    }
   ],
   "source": [
    "from ema_workbench import ema_logging, MultiprocessingEvaluator, Samplers\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "with MultiprocessingEvaluator(model[0]) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=200,               #500\n",
    "                                            policies=4,\n",
    "                                            uncertainty_sampling=Samplers.MC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8,8),\n",
    "                        sharex=True)\n",
    "axes = [axes[0,0],axes[0,1],axes[1,0],axes[1,1]]                             #axes[1,1]\n",
    "\n",
    "robustness_funcs = {\"Expected Number of Deaths\": Expected_Number_of_Deaths,\n",
    "                    \"Expected Annual Damage\": Expected_Annual_Damage,\n",
    "                    \"Total Investment Costs\": Total_Investment_Costs}\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "for ax, (outcome, value) in zip(axes, outcomes.items()):\n",
    "    for policy in np.unique(experiments['policy']):\n",
    "        logical = experiments['policy'] == policy\n",
    "        data = value[logical]\n",
    "        \n",
    "        robustness = []\n",
    "      \n",
    "        for i in range(1, data.shape[0]):\n",
    "            robustness.append(robustness_funcs[outcome](data[0:i]))\n",
    "        ax.plot(robustness, label=policy)\n",
    "    ax.set_xlabel(\"# experiments\")\n",
    "    ax.set_ylabel(outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Search for candidate solutions\n",
    "\n",
    "the fundamental problem is fine tuning the robustness functions. To do this, rather than run optimizaitons many times, why not first generate a test set with a bunch of policies, apply robustness functions and visualize the results?\n",
    "\n",
    "This gives us much faster feedback on reasonble cutoff values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench.em_framework import sample_uncertainties\n",
    "n_scenarios = 10\n",
    "scenarios = sample_uncertainties(model, n_scenarios)\n",
    "nfe = int(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench import (Model, CategoricalParameter,\n",
    "                           ScalarOutcome, IntegerParameter, RealParameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=n_scenarios,              \n",
    "                                            policies=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "sns.violinplot(data=data, y='Total Investment Costs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "ax = sns.violinplot(data=data, y='Expected Annual Damage')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(outcomes)\n",
    "data['policy'] = experiments['policy']\n",
    "ax = sns.violinplot(data=data, y='Expected Number of Deaths')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 1e-5) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1e4) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = functools.partial(robustness, SMALLER, 6e8)#THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "total_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    temp_outcomes = {k:v[logical] for k,v in outcomes.items()}\n",
    "    \n",
    "    for k, v in temp_outcomes.items():\n",
    "        score = funcs[k](v)\n",
    "        scores[k] = score\n",
    "    total_scores[policy] = scores\n",
    "\n",
    "data = pd.DataFrame(total_scores).T.reset_index(drop=True)\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench import (MultiprocessingEvaluator, ema_logging, \n",
    "                           perform_experiments, SequentialEvaluator)\n",
    "from ema_workbench.em_framework.optimization import (HyperVolume, \n",
    "                                                     EpsilonProgress)\n",
    "from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "\n",
    "BaseEvaluator.reporting_frequency = 0.1\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "# there is a bit of problem with platypus, so using 1.1. gives \n",
    "# cleaner hypervolume results.\n",
    "convergence = [HyperVolume(minimum=[0,0,0], maximum=[1.1, 1.1, 1.1]),\n",
    "              EpsilonProgress()]\n",
    "\n",
    "epsilons=[0.05,]*len(robustnes_functions)  #final value of epsilon should be much lower.Just for experiment purposes is 1\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    archive, convergence = evaluator.robust_optimize(robustnes_functions, scenarios,nfe=nfe,\n",
    "                                                     convergence=convergence, epsilons=epsilons)\n",
    "    \n",
    "#start = time.time()\n",
    "#end = time.time()\n",
    "\n",
    "#print('Processing time:',(end-start)/60,'Minutes')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = archive.loc[:, [o.name for o in robustnes_functions]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit better but not much. \n",
    "\n",
    "Now, observe the following: you are using a domain criterion as your sole measure of robustness. That is, you look at the fraction of scenarios above or below a threshold. The costs however do not vary accross scenarios. Thus this objective can only be 0 or 1. This is not particularly useful for optimization. \n",
    "\n",
    "We might thus want to consider another metric for costs. Why not simply use the raw costs itself?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def robustness(direction, threshold, data):\n",
    "    if direction == SMALLER:\n",
    "        return np.sum(data<=threshold)/data.shape[0]\n",
    "    else:\n",
    "        return np.sum(data>=threshold)/data.shape[0]\n",
    "\n",
    "def costs(data):\n",
    "    return data[0]/1e9 # makes numbers nicer\n",
    "    \n",
    "SMALLER = 'SMALLER'\n",
    "\n",
    "Expected_Number_of_Deaths = functools.partial(robustness, SMALLER, 1e-5) #not ok\n",
    "Expected_Annual_Damage = functools.partial(robustness, SMALLER, 1e4) #THOSE NUMBERS NEED TO BE SPECIFIED AGAINS\n",
    "Total_Investment_Costs = costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "experiments, outcomes = results\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "total_scores = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    temp_outcomes = {k:v[logical] for k,v in outcomes.items()}\n",
    "    \n",
    "    for k, v in temp_outcomes.items():\n",
    "        score = funcs[k](v)\n",
    "        scores[k] = score\n",
    "    total_scores[policy] = scores\n",
    "\n",
    "data = pd.DataFrame(total_scores).T.reset_index(drop=True)\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks much nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "MAXIMIZE = ScalarOutcome.MAXIMIZE\n",
    "MINIMIZE = ScalarOutcome.MINIMIZE\n",
    "\n",
    "funcs = {'Expected Number of Deaths':Expected_Number_of_Deaths,\n",
    "         'Expected Annual Damage': Expected_Annual_Damage,\n",
    "         'Total Investment Costs': Total_Investment_Costs}\n",
    "\n",
    "robustnes_functions = [ScalarOutcome('fraction EA deaths', kind=MAXIMIZE, \n",
    "                             variable_name='Expected Number of Deaths', function=Expected_Number_of_Deaths),\n",
    "                       ScalarOutcome('fraction EA damage', kind=MAXIMIZE, \n",
    "                             variable_name='Expected Annual Damage', function=Expected_Annual_Damage),\n",
    "                       ScalarOutcome('investment costs', kind=MINIMIZE, # note that we have to minimize costs!\n",
    "                             variable_name='Total Investment Costs', function=Total_Investment_Costs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# we have to change the plausible max for total investment costs\n",
    "convergence = [HyperVolume(minimum=[0,0,0], maximum=[1.1, 1.1, 3]),\n",
    "              EpsilonProgress()]\n",
    "\n",
    "epsilons=[0.05,]*len(robustnes_functions)  #final value of epsilon should be much lower.Just for experiment purposes is 1\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    archive, convergence = evaluator.robust_optimize(robustnes_functions, scenarios, nfe=nfe,\n",
    "                                                     convergence=convergence, epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, figsize=(8,4))\n",
    "ax1.plot(convergence.nfe, convergence.epsilon_progress)\n",
    "ax1.set_ylabel('$\\epsilon$-progress')\n",
    "ax2.plot(convergence.nfe, convergence.hypervolume)\n",
    "ax2.set_ylabel('hypervolume')\n",
    "\n",
    "ax1.set_xlabel('number of function evaluations')\n",
    "ax2.set_xlabel('number of function evaluations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data = archive.loc[:, [o.name for o in robustnes_functions]]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Re-evaluate candidate solutions under uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench import Policy\n",
    "\n",
    "policies = archive.drop([o.name for o in robustnes_functions], axis=1)\n",
    "policies_to_evaluate = []\n",
    "\n",
    "for i, policy in policies.iterrows():\n",
    "    policies_to_evaluate.append(Policy(\"moro {}\".format(i), **policy.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n_scenarios = 1000\n",
    "with MultiprocessingEvaluator(model) as evaluator:\n",
    "    results = evaluator.perform_experiments(n_scenarios,\n",
    "                                            policies_to_evaluate)\n",
    "\n",
    "#start = time.time()\n",
    "#end = time.time()\n",
    "\n",
    "#print('Processing time:',(end-start)/60,'Minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from ema_workbench import save_results\n",
    "\n",
    "save_results(results, 'MORO_reevaluation.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "policies.to_csv('moro polices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "experiments, outcomes = results\n",
    "\n",
    "overall_robustness = {}\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    policy_robustness = {}\n",
    "\n",
    "    logical = experiments['policy'] == policy\n",
    "    \n",
    "    for outcome, values in outcomes.items():\n",
    "        values = values[logical]\n",
    "        policy_robustness[outcome] = robustness_funcs[outcome](values)\n",
    "    overall_robustness[policy] = policy_robustness\n",
    "overall_robustness = pd.DataFrame.from_dict(overall_robustness).T\n",
    "overall_robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = overall_robustness.loc[:, \n",
    "                              ['Expected Number of Deaths', 'Expected Annual Damage', 'Total Investment Costs']]\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, :] = 0\n",
    "limits.loc[1, :] = 1\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}